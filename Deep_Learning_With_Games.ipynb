{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Deep_Learning_With_Games",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BMALJZcQR6Hk",
        "colab_type": "text"
      },
      "source": [
        "## Python Packages Needed\n",
        "- **Gym** by OpenAI is a library that creates a reinforcement learning environment interface (train models, define different reward functions, different inputs, and visualize performance)\n",
        "- **Stable-Baselines** is a library of popular reinforcement learning algorithms (Deep Q, Actor2Critic, Proximal Policy Optimization, and etc.)\n",
        "- **Gym-Retro** by OpenAI is a library that creates a reinforcement learning environment interface to work with hundreds of different retro games if you have the video game rom (train models in those game environments, define reward functions, and interface with games not already added) "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ppzchq_7W447",
        "colab_type": "text"
      },
      "source": [
        "## Environment Initial Setup\n",
        " I use Google Colab for training since you are able to change the runtime and run off of Google Tesla GPUs. Just go to *Runtime* in the colab menu and select *Change Runtime* then set to **GPU**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XKVhd7QrSZsZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip list #you will see that stable-baselines/gym/tensorflow already installed\n",
        "#pip install stable-baselines\n",
        "#pip install gym"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0BM1Za0SM5t",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 325
        },
        "outputId": "fff7ddf3-b02c-466d-96ba-3e003b924c37"
      },
      "source": [
        "!pip install gym-retro #only install needed in colab"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting gym-retro\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/67/2b/bee76fbe439a8a600854fb41fafcfad7efa57d1f3107bbca48ac4a1387cd/gym_retro-0.7.0-cp36-cp36m-manylinux1_x86_64.whl (162.0MB)\n",
            "\u001b[K     |████████████████████████████████| 162.0MB 260kB/s \n",
            "\u001b[?25hRequirement already satisfied: pyglet==1.*,>=1.3.2 in /usr/local/lib/python3.6/dist-packages (from gym-retro) (1.4.3)\n",
            "Requirement already satisfied: gym in /usr/local/lib/python3.6/dist-packages (from gym-retro) (0.10.11)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from pyglet==1.*,>=1.3.2->gym-retro) (0.16.0)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.3.1)\n",
            "Requirement already satisfied: requests>=2.0 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (2.21.0)\n",
            "Requirement already satisfied: numpy>=1.10.4 in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.16.5)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from gym->gym-retro) (1.12.0)\n",
            "Requirement already satisfied: urllib3<1.25,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (1.24.3)\n",
            "Requirement already satisfied: chardet<3.1.0,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2019.6.16)\n",
            "Requirement already satisfied: idna<2.9,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.0->gym->gym-retro) (2.8)\n",
            "Installing collected packages: gym-retro\n",
            "Successfully installed gym-retro-0.7.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GCvn0QLHWLPm",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 127
        },
        "outputId": "646ef054-af53-49d1-aec3-981625d8bf0e"
      },
      "source": [
        "from google.colab import drive #using Google Drive for my storage File system since using Google Colab to train\n",
        "drive.mount('/content/drive') #you should see a drive folder appear in the left sidebar"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=email%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdocs.test%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive.photos.readonly%20https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fpeopleapi.readonly&response_type=code\n",
            "\n",
            "Enter your authorization code:\n",
            "··········\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdOd0BKUYKe_",
        "colab_type": "text"
      },
      "source": [
        "## Imports Needed\n",
        "- Gym\n",
        "- Numpy\n",
        "- Gym-Retro (as retro)\n",
        "- Stable-Baselines (specific models needed -> https://stable-baselines.readthedocs.io/en/master/)\n",
        "***You can also use a self built model or other models! Then you wouldn't need to import Stable-Baselines**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vlsfWdIEWYTA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import gym \n",
        "import numpy as np\n",
        "import retro\n",
        "from stable_baselines.common.policies import CnnPolicy\n",
        "from stable_baselines.common.vec_env import DummyVecEnv, VecNormalize\n",
        "from stable_baselines import A2C , PPO2"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5AQxTRdOYL1f",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 793
        },
        "outputId": "e70c4670-7e76-4efe-8bef-9927cd67dc9d"
      },
      "source": [
        "!python -m retro.import.sega_classics"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Steam Username: phoenixtechnerd\n",
            "Steam Password (leave blank if cached): \n",
            "Steam Guard code: 4VB45\n",
            "Downloading games...\n",
            "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4' from LD_PRELOAD cannot be preloaded (wrong ELF class: ELFCLASS64): ignored.\n",
            "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4' from LD_PRELOAD cannot be preloaded (wrong ELF class: ELFCLASS64): ignored.\n",
            "ERROR: ld.so: object '/usr/lib/x86_64-linux-gnu/libtcmalloc.so.4' from LD_PRELOAD cannot be preloaded (wrong ELF class: ELFCLASS64): ignored.\n",
            "Installing games...\n",
            "Importing SonicTheHedgehog-Genesis\n",
            "Importing BioHazardBattle-Genesis\n",
            "Importing SuperThunderBlade-Genesis\n",
            "Importing AlienSoldier-Genesis\n",
            "Importing GoldenAxe-Genesis\n",
            "Importing StreetsOfRage2-Genesis\n",
            "Importing SonicAndKnuckles3-Genesis\n",
            "Importing ShadowDancerTheSecretOfShinobi-Genesis\n",
            "Importing ColumnsIII-Genesis\n",
            "Importing ComixZone-Genesis\n",
            "Importing GainGround-Genesis\n",
            "Importing CrackDown-Genesis\n",
            "Importing RevengeOfShinobi-Genesis\n",
            "Importing KidChameleon-Genesis\n",
            "Importing DynamiteHeaddy-Genesis\n",
            "Importing GalaxyForceII-Genesis\n",
            "Importing SonicTheHedgehog2-Genesis\n",
            "Importing AlteredBeast-Genesis\n",
            "Importing Flicky-Genesis\n",
            "Importing ShinobiIIIReturnOfTheNinjaMaster-Genesis\n",
            "Importing Columns-Genesis\n",
            "Importing DrRobotniksMeanBeanMachine-Genesis\n",
            "Importing BonanzaBros-Genesis\n",
            "Importing Vectorman2-Genesis\n",
            "Importing SpaceHarrierII-Genesis\n",
            "Importing Vectorman-Genesis\n",
            "Importing Ristar-Genesis\n",
            "Importing GoldenAxeIII-Genesis\n",
            "Importing StreetsOfRage-Genesis\n",
            "Importing MonsterLair-Genesis\n",
            "Importing StreetsOfRage3-Genesis\n",
            "Importing ToeJamAndEarlInPanicOnFunkotron-Genesis\n",
            "Importing FatalLabyrinth-Genesis\n",
            "Imported 33 games\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jQ7F2pqadmw",
        "colab_type": "text"
      },
      "source": [
        "## Making the Action Space More Discrete\n",
        "For *Gym-Retro* the standard output array (actions for the agent to choose) is has a length of 12 and is defined as follows\n",
        "- [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
        "\n",
        "To reduce training time I recommend adjusting the action space to fit the specific game. Here is an example from Sonic of creating array items and sub-arrays for actions that can be taken together."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oWYN5YO6bfaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SonicDiscretizer(gym.ActionWrapper):\n",
        "    \"\"\"\n",
        "    Wrap a gym-retro environment and make it use discrete\n",
        "    actions for the Sonic game.\n",
        "    \"\"\"\n",
        "    def __init__(self, env):\n",
        "        super(SonicDiscretizer, self).__init__(env)\n",
        "        buttons = [\"B\", \"A\", \"MODE\", \"START\", \"UP\", \"DOWN\", \"LEFT\", \"RIGHT\", \"C\", \"Y\", \"X\", \"Z\"]\n",
        "        actions = [['LEFT'], ['RIGHT'], ['LEFT', 'DOWN'], ['RIGHT', 'DOWN'], ['DOWN'],['DOWN', 'B'], ['B']]\n",
        "        self._actions = []\n",
        "        for action in actions:\n",
        "            arr = np.array([False] * 12)\n",
        "            for button in action:\n",
        "                arr[buttons.index(button)] = True\n",
        "            self._actions.append(arr)\n",
        "        self.action_space = gym.spaces.Discrete(len(self._actions))\n",
        "\n",
        "    def action(self, a):\n",
        "      return self._actions[a].copy()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LPXv1WBQb1Th",
        "colab_type": "text"
      },
      "source": [
        "## Creating Your First Retro Environment\n",
        "\n",
        "To see if a particular retro game is supported visit (https://github.com/openai/retro/tree/master/retro/data/stable)\n",
        "\n",
        "\n",
        "***This folder contains 4 primary files along with the different level/game states***\n",
        "- **Scenario.json** (which is where you define your reward function)\n",
        "- **data.json** (which defines the memory address in the rom for different variables usable in the Scenario.json file)\n",
        "- **metadata.json** (which provides a default state and you probably won't touch much)\n",
        "- **rom.sha** (which is the hash of the rom being used and must corespond with the game rom you have)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s_GqQ3YDeiAI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = retro.make(game='SonicTheHedgehog2-Genesis', state='EmeraldHillZone.Act1', scenario='/content/updatedScenario.json')\n",
        "env = DummyVecEnv([lambda: SonicDiscretizer(env)]) #note that the vecotrized environment is wrapped in the descritizer. This is only needed if you discrete the action space"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1rv4Ewbce_Z5",
        "colab_type": "text"
      },
      "source": [
        "## Creating Your Own Scenario File\n",
        "\n",
        "You may have noticed in the above *retro.make* function that the keyword argument *scenario=* has a file passed in. This is how you update your reward function using the available variables in the **data.json** file. \n",
        "\n",
        "If you are using Google Colab you can upload this file into the notebook and it will be at *content/*filename\n",
        "\n",
        "```\n",
        "{\n",
        "  \"done\": {\n",
        "    \"variables\": {\n",
        "      \"lives\": {\n",
        "        \"op\": \"zero\"\n",
        "      }\n",
        "    }\n",
        "  },\n",
        "  \"reward\": {\n",
        "    \"variables\": {\n",
        "      \"score\": {\n",
        "        \"reward\": 3\n",
        "      },\n",
        "      \"rings\": {\n",
        "        \"reward\": 50.0\n",
        "      }\n",
        "    }\n",
        "  }\n",
        "}\n",
        "```\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K8QdKJffglvL",
        "colab_type": "text"
      },
      "source": [
        "## Using Stable-Baselines Models \n",
        "\n",
        "In this demo I am using a Proximal Policy Optimization Network (https://openai.com/blog/openai-baselines-ppo/) but other stable-baselines models such as A2C or DQN can be used. \n",
        "\n",
        "\n",
        "***Also, you can use other models or your own models in a similar way! ***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LAiMhT9MgHg_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_id = 'sonic2ppo2dis'\n",
        "model = PPO2(CnnPolicy, env, n_steps=2048, verbose=1) #define specific model, also notice CNN as the input\n",
        "model.learn(total_timesteps=500000) #total timesteps to train the model. Google Colab you can do about 1 to 1.5 million at a time\n",
        "model.save('/gdrive/My Drive/Sonic2-Discrete/' + model_id) #save model as a pkl file to either load and continue training or use trained model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yY86shsRintY",
        "colab_type": "text"
      },
      "source": [
        "## Loading A Model to Continue Training, Update the Reward Function, or Change State\n",
        "\n",
        "Given the pkl file that gym saves you are able to load in different reward functions, game states, or just train longer on Google Colab (about 1 million - 1.5 million timesteps before you use the allocated memory)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yMK5IvkTjiSe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "env = retro.RetroEnv(game='SonicAndKnuckles3-Genesis', state='AngelIslandZone.Act1', record='/gdrive/My Drive/Sonic2/')\n",
        "env = DummyVecEnv([lambda: SonicDiscretizer(env)])\n",
        "model = PPO2.load('/gdrive/My Drive/Sonic2Speedrun/sonic2Speedrun_TransferGreedyContest_7-3mil', env)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s6c6ccwikRMC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model_id = 'sonic3Speedrun_TransferGreedyContest_3mil'\n",
        "model.learn(total_timesteps=1000000)\n",
        "model.save('/gdrive/My Drive/Sonic3/' + model_id)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4ACCRD-1klQ3",
        "colab_type": "text"
      },
      "source": [
        "## Prediciting with Your Model\n",
        "\n",
        "Reinforcement Algorithms are focused on exploration and exploitation. When learning exploration is critical with the optimal policy still being utilized to exploit. When you are predicting the model is just exploiting and not exploring the environment for finding other optimal outcomes. \n",
        "\n",
        "***If you are predicting make sure you are loaded with a path for the record parameter to visualize the model***"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wlohvdQXk3pN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "obs = env.reset()\n",
        "done = False\n",
        "\n",
        "while not done:\n",
        "    action, _info = model.predict(obs) #predicting with the model\n",
        "    obs, rewards, dones, info = env.step(action) #variables under the hood"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dUwiqmAOl7KB",
        "colab_type": "text"
      },
      "source": [
        "## What Does My Model See\n",
        "\n",
        "- ***Obs***: variable that contains the input observation (in this case the screen itself as pixel values)\n",
        "- ***Rewards***: intermitent rewards that the agent is being fed defined in the scenario.file\n",
        "- ***Dones***: whether the done condition (also in scenrio file) was met\n",
        "- ***Info***: all variables defined in the data.json and their current values"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Heg3uamfnDdB",
        "colab_type": "text"
      },
      "source": [
        "## Sweet Now I Got a BK2 File??\n",
        "To convert your bk2 file (which is an array of actions or buttons that were selected) to a more viewable format like mp4 there is a built in function provided with Retro"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FaOfZo9HjsZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!python -m retro.scripts.playback_movie /content/drive/My\\ Drive/SOR3_Dis/StreetsOfRage3-Genesis-1Player.Axel.Stage1-000001.bk2 #notice the escape character on My\\ Drive"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}